\documentclass{article}

\usepackage{times}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=.5in]{geometry}
\usepackage{graphicx}
\usepackage[linewidth=1pt]{mdframed}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\title{Machine Learning and Data Mining - Homework 1}
\author{Philip Warton}
\date{\today}
\maketitle
\section{Statistical Estimation}
    \subsection{Q1}
        \begin{mdframed}[]
            Let $D = \{x_1,\cdots,x_N\}$ be a dataset from $N$ poisson random variables, with a rate 
            of $\lambda \in \mathbb{R}$. Derive the Maximum Likelihood Estimation for $\lambda$.
        \end{mdframed}
        We begin by taking $\mathcal{L}(D)$. That is,
        \begin{align*}
            \mathcal{L}(D) &= P(D \ | \ \lambda) \\
            &=P(\{x_1,\cdots,x_N\} \ | \ \lambda) \\
            &=P(x_1 \ | \ \lambda ) \cdots P(x_N \ | \ \lambda) \\
            &= \prod_{i = 1}^NP(x_i \ | \ \lambda) \\
            &= \prod_{i = 1}^N \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}
        \end{align*}
        Then to get the log-likelihood, we take $\ln \mathcal{L}(D)$.
        \begin{align*}
            \ln\mathcal{L}(D) &= \ln \prod_{i = 1}^N \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}\\
            &= \sum_{i=1}^N \ln \left( \frac{\lambda^{x_i}e^{-\lambda}}{x_i!} \right) \\
            &= \sum_{i=1}^N \left[
                \ln(\lambda^{x_i}e^{-\lambda}) - \ln(x_i!)
            \right] \\
            &= \sum_{i=1}^N \left[
                \ln(\lambda^{x_i}) + \ln(e^{\lambda}) - \sum_{j=1}^i\ln(x_j)
            \right]\\
            &= \sum_{i = 1}^N \left[
                x_i \ln \lambda - \lambda \ln e - \sum_{j = 1}^i \ln x_j
            \right]\\
            &=\ln \lambda \sum_{i = 1}^Nx_i - N\lambda - \sum_{i = 1}^N \sum_{j = 1}^i \ln x_j
        \end{align*}
        We now take the derivative of the log-likelihood, giving us 
        \begin{align*}
            \frac{d}{d\lambda}\left(
                \ln \mathcal{L}(D)
            \right) &= \frac{d}{d\lambda}\left(
                \ln \lambda \sum_{i = 1}^Nx_i - N\lambda - \sum_{i = 1}^N \sum_{j = 1}^i \ln x_j
            \right) \\
            &=\frac{1}{\lambda} \sum_{i=1}^N x_i - N - 0
        \end{align*}
        Let this derivative be equal to zero. Then we have 
        \begin{align*}
            0 &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N \\
            N &= \frac{1}{\lambda} \sum_{i = 1}^N x_i \\
            \lambda &= \frac{1}{N} \sum_{i = 1}^N x_i \\
            \lambda &= \overline{x}
        \end{align*}
    \subsection{Q2}
        \begin{mdframed}[]
            Poo Poo
        \end{mdframed}
        We take the derivative of the log posterior.
        \begin{align*}
            (d/d\lambda) \log P(\lambda \ | \ D) &= (d / d\lambda) c\log P(D \ | \ \lambda) + \log \left(\text{Gamma of lambda}\right) \\
            &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N + \frac{d}{d\lambda }\left( \ln \left( \frac{\beta^\alpha \lambda^{\alpha - 1}e^{-\beta\lambda}}{\Gamma(\alpha)}\right) \right) \\
            &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N + \frac{d}{d\lambda }\bigg( 
                \ln(\beta^\alpha) + \ln(\lambda^{\alpha-1}) + \ln(e^{-\beta\lambda}) - \ln(\Gamma(\alpha))
            \bigg)\\
            &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N + \frac{d}{d\lambda }\bigg( 
                \alpha \ln(\beta) + (\alpha - 1)\ln(\lambda) + -\beta\lambda - \ln(\Gamma(\alpha))
            \bigg)\\
            &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N + \bigg( 
                0 + (\alpha - 1)\frac{1}{\lambda} + -\beta - 0
            \bigg) \\
            &= \frac{1}{\lambda}\sum_{i=1}^N x_i - N + \frac{\alpha - 1}{\lambda} + -\beta \\
            &= \frac{1}{\lambda} \left( 
                \left(\sum_{i=1}^N x_i\right)  + \alpha - 1 
            \right)
            - (N + \beta)
        \end{align*}
        Now let this derivative be equal to 0. Then,
        \begin{align*}
            0 & = \frac{1}{\lambda} \left( 
                \left(\sum_{i=1}^N x_i\right)  + \alpha - 1 
            \right)
            - (N + \beta) \\
            N + \beta &= \frac{1}{\lambda} \left( 
                \left(\sum_{i=1}^N x_i\right)  + \alpha - 1 
            \right) \\
            \lambda &= \frac{1}{N + \beta} \left( 
                \left(\sum_{i=1}^N x_i\right)  + \alpha - 1 
            \right) \\
            \lambda &= \frac{\left(\sum_{i=1}^N x_i\right) + \alpha - 1}{N + \beta}
        \end{align*}
        And thus we have a closed form solution for our parameter $\lambda$.
\section{k-Nearest Neighbor Classifier}
    \subsection{Q4}
        Consider the case where we have one sample $x$ such that workclass$[x] = $ Private 
        and where all other variables are 0. Then we have another sample $y$ such that 
        workclass$[y] = $ Never-worked and all other variables are 0. Within our first encoding, we have 
        \[
            ||x - y|| = \sqrt{ 0 + \cdots + 1^2 + 0 + 1^2 + \cdots + 0} = \sqrt{2}
        \]
        Whereas in the second encoding, where we simply keep one column but associate different integers with category we would have 
        \[
            ||x - y|| = \sqrt{0 + \cdots + (3 - 1)^2 + \cdots + 0} = 2
        \]
        This discrepancy will grow larger as the number of categories in a given column grows larger, and 
        what we see is certain categorical differences will be a greater difference than other categorical differences
        within that same column. For this reason, we prefer binarization so that the categories are evenly spaced,
        even though we may have to work in a higher dimension.
    \subsection{Q5}
    We use the following code to compute the percentage of people with an income larger than 50 thousand dollars per year:
        \begin{verbatim}
def printPercentOver50k(D):
    k = 0
    for x in D:
        if (x[85] == 1):
            k += 1
    n = 8000
    print(100 * (k / n))

printPercentOver50k(D)
            \end{verbatim}
            This gives us a result of $24.5875 \%$. We know that $70\%$ accuracy will not be very good because we could achieve 
            this accuracy by classifying each datapoint as 0. We want our model to have a much higher accuracy than this.
    \subsection{Q6}
        \begin{proof}
            Let $x, z \in \mathbb{R}^n$.
            \begin{align*}
                d(x, z) &= \sqrt{(x_1 - z_1)^2 + (x_2 - z_2)^2 + \cdots + (x_n - z_n)^2} \\
                &= \sqrt{\sum_{i = 1}^n (x_i - z_i)^2} \\
                &= ||x - z||_2
            \end{align*}
        \end{proof}
\end{document}