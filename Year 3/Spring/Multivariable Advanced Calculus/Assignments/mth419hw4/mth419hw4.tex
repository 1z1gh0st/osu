\documentclass{article}

\usepackage{times}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[margin=.5in]{geometry}
\usepackage{graphicx}
\usepackage[linewidth=1pt]{mdframed}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{bm}

\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\title{Advanced Multivariable Calculus - Homework 4}
\author{Philip Warton}
\date{\today}
\maketitle
\section*{Problem 1}
\begin{mdframed}[]
    Let $G \subset \mathbb{R}^n$ be open and let $f:G \rightarrow \mathbb{R}$ and suppose that 
    $\partial_{x_i}$ exists and is bounded for every $1 \leqslant i \leqslant n$. Show that 
    $f$ is continuous.
\end{mdframed}
\begin{proof}
    Let $\vec x \in G$ be a point arbitrarily.
    The since $G$ is open we know that some $\epsilon$-neighborhood of $\vec x$ is contained in $G$.
    Let $\epsilon > 0$ be arbitrary. To show that there exists some $\delta > 0$ such that 
    $\vec y \in B_\delta(\vec x) \Rightarrow f(\vec y) \in B_\epsilon(f(\vec x))$ would suffice to show that 
    $f$ is continuous by the definition of continuity. Define 
    \[
        M = \max_{i \in \{1,2,\cdots,n\}}\left\{M_i\right\}
    \]
    Where $M_i = \sup_{\vec g \in G}\{\partial_{x_i}\}$. Then define $0 < \delta < \frac{\epsilon}{nM}$ such that $B_\delta(\vec x) \subset G$.
    Write $\vec x = (x_1, x_2, \cdots , x_n)$ and let $\vec y \in B_\delta(\vec x)$. 
    Let $\vec u_k = (y_1, \cdots, y_k, x_{k+1},\cdots,x_n)$ for $k = 1,2,\cdots,n$.
    Then,
    \begin{align*}
        |f(\vec x) - f(\vec y)| &=|f(\vec x) - f(\vec u_1) + f(\vec u_1) - f(\vec u_2) + f(\vec u_2) - \cdots - f(\vec u_k) + f(\vec u_k) - f(\vec y)|\\
        &\leqslant |f(\vec x) - f(\vec u_1)| + |f(\vec u_1) - f(\vec u_2)| + \cdots + |f(\vec u_k) - f(\vec y)|\\
        &\leqslant M|x_1-y_1| + M|x_2 - y_2| + \cdots + M|x_n - y_n| \\
        &\leqslant nM\delta \\
        &= nM\frac{\epsilon}{nM} = \epsilon
    \end{align*}
    We know that $|f(\vec u_{i+1}) - f(\vec u_{i+2})| \leqslant M|x_1 - x_2|$ by the mean value theorem,
    applied to $f$ in the $i$-th plane at our point $\vec x$.
\end{proof}
\section*{Problem 2}
\begin{mdframed}[]
    Let 
    \[
        f(x,y) = \begin{cases}
            \frac{x^3}{x^2 + y^2} & (x,y) \neq \bm 0 \\
            0 & (x,y) = \bm 0
        \end{cases}
    \]
\end{mdframed}
\subsection*{a)}
Show that $\partial_x, \partial_y$ exist and are bounded.\\\\
Suppose that $(x,y) \neq \bm 0$. Then we write 
\begin{align*}
    \frac{\partial f}{\partial x} & = \frac{(x^2 + y^2)(3x^2) - (x^3)(2x)}{(x^2 + y^2)^2} = \frac{x^4 + 3x^2y^2}{x^4 + 2x^2y^2 + y^4} \\\\
    \frac{\partial f}{\partial y} & = \frac{(x^2 + y^2)(0) - (x^3)(2y)}{(x^2 + y^2)^2} = \frac{-x}{x^2+y^2}\cdot\frac{2xy}{x^2+y^2}
\end{align*}
Then if $(x,y) = \bm 0$ simply write $\partial_x(\bm 0) = 1, \partial_y(\bm 0) = 0$ by computing $\lim_{h\rightarrow 0}\frac{f(h,0) - f(0,0)}{h}$ and  $\lim_{h\rightarrow 0}\frac{f(0,1) - f(0,0)}{h}$. To show that these are bounded we write 
\begin{align*}
    \frac{\partial f}{\partial x} &= \frac{x^4}{x^4 + 2y^2x^2 + y^4} + \frac{3x^2y^2}{x^2 + 2x^2y^2 + y^4}\\
    &\leqslant \frac{x^4}{x^4} + {3x^2y^2}{2x^2y^2} \\
    &\leqslant 1 + \frac{3}{2}
\end{align*}
Because every term is squared and positive, it follows that $0 \leqslant \partial_x \leqslant \frac{5}{2}$ and is bounded.
For our $y$ partial, we write
\begin{align*}
    0 \leqslant \frac{x^2}{x^2 + y^2} \leqslant 1
\end{align*}
Then we say that 
\begin{align*}
    (|y| - |x|)^2 & \geqslant 0 \\
    y^2 - 2|y||x| + x^2 &\geqslant 0 \\
    y^2 + x^2 &\geqslant 2|x||y| \\
    1 &\geqslant \frac{2|x||y|}{x^2+y^2} \\
    1 &\geqslant \left|\frac{2xy}{x^2+y^2}\right| \\
\end{align*}
So then we have $\partial_y = \frac{x^2}{x^2 + y^2} \cdot \frac{2xy}{x^2 + y^2}$ is a product of bounded functions, and is therefore bounded.
\subsection*{b)}
Show that $\nabla_{\bm u}(f)$ exists at $\bm 0$ for every unit vector $\bm u$.\\\\
Denote $\bm u = (u_1,u_2)$. We compute the directional derivative as 
\begin{align*}
    \lim_{h \rightarrow 0} \frac{f(\bm 0 + h \bm u) - f(\bm 0)}{h} &= \lim_{h \rightarrow 0} \frac{\frac{hu_1^3}{u_1^2 + u_2^2}}{h} \\
    &= \lim_{h \rightarrow 0} \frac{u_1^3}{u_1^2 + u_2^2}
\end{align*}
We know that $u_1^2 + u_2^2 = 1$ since $\bm u$ is a unit vector, and we know that $|u_1| \leqslant 1$ for the same reason.
So it follows that $|\nabla_{\bm u}f(\bm 0)| < 1$.
\subsection*{c)}
Choose $\bm u = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})$. Then we have
\[
    \nabla_{\bm u}f(\bm 0) = \frac{\left(\frac{1}{\sqrt{2}}\right)^3}{\frac{1}{\sqrt{2}}^2 + \frac{1}{\sqrt{2}}^2} = \left(\frac{1}{\sqrt{2}}\right)^3 = 2^{-3/2}
\]
However, note that 
\[
    \nabla f (\bm 0 ) \cdot \bm u = \begin{pmatrix}
        1 \\ 0
    \end{pmatrix} \cdot \begin{pmatrix}
        \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
    \end{pmatrix} = \frac{1}{\sqrt{2}} \neq \frac{1}{\sqrt{2}^3}
\]
\section*{Problem 3}
\begin{mdframed}[]
    Let$G \subset \mathbb{R}^n$ be  an  open  set,  and  assume$f:G\rightarrow \mathbb{R}$ is  differentiable  on $G$.
    Also assume $f$ has a local maximum at $x\in G$. Prove that $\nabla f(x) =0$.
\end{mdframed}
\begin{proof}
    Write $\bm x = (a_1,a_2,\cdots,a_n)$. Now write 
    \[
        f_i(t) = f(a_1,\cdots,a_{i-1},t,a_{i+1},\cdots,a_n)
    \]
    For every $i = 1,2,\cdots,n$. If $t \in (-\epsilon + a_i, a_i + \epsilon)$
    then $(a_1,\cdots,a_{i-1},t,a_{i+1},\cdots,a_n) \in B_\epsilon(\bm x)$. So it follows that 
    for each such $t$ in the $\epsilon$-neighborhood of $a_i, f_i(t) \leqslant f(\bm x) = f_i(a_i)$. Thus we have that $t = x_i$
    is a local maximum for $f_i$, and therefore \[f_i'(a_i) = \frac{\partial f}{\partial x_i}(\bm x) = \bm 0\]
    Since $f$ is differentiable, we know that each partial derivative exists, and it must be the case 
    that these partial derivatives for $x_i$ are equal to the derivative of the restricted function $f_i$ at the point $\bm x$
    by definition of the partial derivative.
\end{proof}
\section*{Problem 4}
Let $f(x,y) = (e^x \cos y, e^x \sin y)$. 
\subsection*{a)}
\begin{mdframed}[]
    Compute the Jacobian $J_f$.
\end{mdframed}
We are going to compute the appropriate derivatives as
\begin{align*}
    \frac{\partial}{\partial x}e^x \cos y &= e^x \cos y \\\\
    \frac{\partial}{\partial y}e^x \cos y &= -e^x \sin y \\\\
    \frac{\partial}{\partial x}e^x \sin y &= e^x \sin y \\\\
    \frac{\partial}{\partial y}e^x \sin y &= e^x \cos y 
\end{align*}
Then our matrix will clearly be 
\[
    J_f = \begin{pmatrix}
        e^x \cos y & -e^x \sin y\\
        e^x \sin y & e^x \cos y 
    \end{pmatrix}
\]
\subsection*{b)}
\begin{mdframed}[]
    Show that $J_f(x,y)^{-1}$ exists for every point in $\mathbb{R}^2$.
\end{mdframed}
\begin{proof}
We know that $J_f(x,y)^{-1}$ exists if and only if $\det (J_f(x,y)) \neq 0$. So we compute 
    \begin{align*}
        \det (J_f(x,y)) & = (e^x \cos y)(e^x \cos y) - ((-e^x) \sin y)(e^x \sin y) \\
        &= (e^x)^2(\cos^2 y) + (e^x)^2(\sin^2 y) \\
        &= (e^x)^2(\cos^2 y + \sin^2 y) \\
        &= (e^x)^2
    \end{align*}
    Since $e^x$ is strictly positive, so is its square, so obviously $\det(J_f(x,y)) = (e^x)^2 > 0$
    and we say that the matrix is invertible.
\end{proof}
\subsection*{c)}
\begin{mdframed}[]
    Show that $f$ is not invertible globally; that is, find different points of the domain
    that are mapped to the same value in the range.
\end{mdframed}
\begin{proof}
    Since $e^x$ is a bijective map, we fix $x = 0$. Then we know that trigonometric functions 
    are $2\pi$-periodic, so pick the following two points $\bm a = (0,0), \bm b = (0,2\pi)$.
    Then we know that $f(\bm a) = (1, 0)$ and $f(\bm b) = (1, 0)$. So we know that the pre-image 
    of $(1,0)$ includes both $\bm a$ and $\bm b$ so then $f^{-1}$ is not globally invertible.
\end{proof}
\section*{Problem 5}
\begin{mdframed}[]
    Let $f_1(x,y_1,y_2) = 3y_1 +y_2^2 +4x = 0, f_2( x , y_1 , y_2 ) = 4 y^3_1 + y_2 + x = 0$.
\end{mdframed}
\subsection*{a)}
Verify that $(x,y_1,y_2) = (-4,0,4)$ is a solution.\\\\
We have that 
\[
    f_1(-4,0,4) = 3(0) + (4)^2 + 4(-4) = 16 - 16 = 0
\]
And also that 
\[
    f_2(-4,0,4) = 4(0)^3 + (4) + (-4) = 0
    \]
    So then we know that $(-4,0,4)$ is a valid solution.
\subsection*{b)}
Verify that $\exists g = (g_1,g_2)$ such that $g_1(x) = y_1, g_2(x) = y_2$ for all solutions near $(-4,0,4)$.\\\\
We write 
\begin{align*}
    \begin{pmatrix}
        \frac{\partial f_i}{\partial y_j}
    \end{pmatrix} & = \begin{pmatrix}
        3 & 2y_2\\
        12y_1^2 & 1
    \end{pmatrix}
\end{align*}
Then because the determinant is equal to $3 - (12y_1^2)(2y_2)$, if we plug in $\bm s = (-4, 0 ,4)$
we will get $\det\left(\frac{\partial f_i}{\partial y_j}\right) = 3 \neq 0$. So by the implicit function 
theorem it follows that $\exists g:\mathbb{R} \rightarrow \mathbb{R}^2$ such that for every $(x,y_1,y_2) \in S \cap B_\epsilon(-4,0,4)$
where $S$ is the set of solutions to our system of equations $(x,y_1,y_2) = (x,g(x)^t)$ therefore $g_1(x) = y_1$ and $g_2(x) = y_2$.
\subsection*{c)}
So we use the final piece of the implicit function theorem for solutions around $(-4,0,4)$ since 
\begin{align*}
    \begin{pmatrix}
        4\\1
    \end{pmatrix} + \begin{pmatrix}
        3 & 8 \\ 0 & 1
    \end{pmatrix}\begin{pmatrix}\frac{\partial g_1}{\partial x} \\ \frac{\partial g_2}{\partial x}\end{pmatrix} & = \begin{pmatrix}
        0 \\ 0
    \end{pmatrix}
\end{align*}
So it follows that $\frac{\partial g_2}{\partial x} = -1$ and $\frac{\partial g_1}{\partial x} = \frac{4}{3}$.
\section*{Problem 6}
\begin{mdframed}
    Let $T = \{(x,y,u,v) \in \mathbb{R}^4 \ | \ x^2 + y^2 = 1, \ ux + vy = 0\}$. Then let 
    \[
        f(x_1,x_2,y_1,y_2) = (x_1^2+y_1^2, x_2x_1 + y_2y_1)
    \]
    Where $x_1 = x, y_1 = y, x_2 = u, y_2 = v$. Then we write $T = f^{-1}(1,0)$.
\end{mdframed} 
\subsection*{a)}
Compute $\frac{\partial f_i}{\partial y_j}$.
We simply take each of our partial derivatives,
\begin{align*}
    \begin{pmatrix}
        \frac{\partial f_i}{\partial y_j}
    \end{pmatrix} &= \begin{pmatrix}
        \frac{\partial f_1}{\partial y_1}& \frac{\partial f_1}{\partial y_2}\\\\
        \frac{\partial f_2}{\partial y_1} & \frac{\partial f_2}{\partial y_2}
    \end{pmatrix} \\\\
    &= \begin{pmatrix}
        2y_1 & 0 \\
        y_2 & y_1
    \end{pmatrix}
\end{align*}
\subsection*{b)}
We know that $\det \left(\frac{\partial f_i}{\partial y_j}\right) = 2y_1^2$, so it follows that if $y_1 = y \neq 0$
then this determinant is non zero. So there exists by the implicit function theorem a continuously differentiable function $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$
such that for all $(x_1,x_2,y_1,y_2) \in T$ we can write 
\[
    (x_1,x_2,y_1,y_2) = (x_1,x_2,g(x_1,x_2))
\]
which is entirely in terms of $x$ and $u$ since $x = x_1, u = x_2$.
\subsection*{c)}
Find formulas for $y$ and $v$ in terms of $x$ and $u$ valid when $y \neq 0$.\\\\
If $y \neq 0$ then we can write $y = \pm \sqrt{1-x^2}$ and $v = \frac{ux}{y} = \frac{ux}{\pm \sqrt{1-x^2}}$.
Then we know that our denominator is non-zero and we have a formula for $y$ and $v$ all in terms of $u$ and $x$.
\subsection*{d)}
If $x \neq 0$ then $x_1 \neq 0$ so we write $x = \pm \sqrt{1-x^2}$ and $u = \frac{vy}{x} = \frac{vy}{\sqrt{1-x^2}}$.
Since the denominator is guaranteed to be non-zero we have written $x,u$ in terms of $y,v$.
\end{document}